{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb623e4d",
   "metadata": {},
   "source": [
    "# Genetic algorithm experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebc9f1",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f85c5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d3e22",
   "metadata": {},
   "source": [
    "#### Ray initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab43d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = os.path.abspath(\"../src\")\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6b0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    ray.init(\n",
    "        runtime_env={\"env_vars\": {\"PYTHONPATH\": os.environ[\"PYTHONPATH\"]}},\n",
    "        include_dashboard=True,\n",
    "        dashboard_port=8265,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a8e51",
   "metadata": {},
   "source": [
    "#### Problem loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eabf62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from model import Problem\n",
    "from utils import load_from_json\n",
    "\n",
    "data_dir = \"problems\"\n",
    "json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "\n",
    "data: list[tuple[str, Problem]] = []\n",
    "for json_file in json_files:\n",
    "    testcase_name = os.path.basename(json_file).replace(\".json\", \"\")\n",
    "    problem = load_from_json(json_file)\n",
    "    data.append((testcase_name, problem))\n",
    "\n",
    "data.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a080d18",
   "metadata": {},
   "source": [
    "#### Setup results directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c935e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results\"\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05743995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, directory, filename_suffix):\n",
    "    df = pd.DataFrame(results)\n",
    "    testcase_name = df[\"testcase\"].iloc[0]\n",
    "    filename = os.path.join(directory, f\"{testcase_name}_{filename_suffix}.csv\")\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ab155",
   "metadata": {},
   "source": [
    "#### Setup all mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c06b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ga.mutations import (\n",
    "    CouriersMutation,\n",
    "    Mutation,\n",
    "    NewCourierMutation,\n",
    "    PackagesMutation,\n",
    "    RouteMutation,\n",
    "    UnusedVehiclesMutation,\n",
    "    UsedVehiclesMutation,\n",
    ")\n",
    "\n",
    "MUTATIONS: list[Mutation] = [\n",
    "    UsedVehiclesMutation,\n",
    "    UnusedVehiclesMutation,\n",
    "    CouriersMutation,\n",
    "    PackagesMutation,\n",
    "    RouteMutation,\n",
    "    NewCourierMutation,\n",
    "]\n",
    "\n",
    "MUTATION_ALIAS = {\n",
    "    \"CouriersMutation\": \"CM\",\n",
    "    \"UsedVehiclesMutation\": \"UsedVM\",\n",
    "    \"UnusedVehiclesMutation\": \"UnusedVM\",\n",
    "    \"NewCourierMutation\": \"NewCM\",\n",
    "    \"PackagesMutation\": \"PM\",\n",
    "    \"RouteMutation\": \"RM\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f32870",
   "metadata": {},
   "source": [
    "#### Genetic algorithm loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0c55d",
   "metadata": {},
   "source": [
    "##### Inital configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5476995",
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_RUN_PATIENCE = 100\n",
    "GA_INITIAL_POPULATION_SIZE = 50\n",
    "GA_MAX_RUN_ITERATIONS = 300\n",
    "GA_RUN_REPEAT = 10\n",
    "\n",
    "CONFIG = {\n",
    "    \"GA_RUN_PATIENCE\": GA_RUN_PATIENCE,\n",
    "    \"GA_INITIAL_POPULATION_SIZE\": GA_INITIAL_POPULATION_SIZE,\n",
    "    \"GA_MAX_RUN_ITERATIONS\": GA_MAX_RUN_ITERATIONS,\n",
    "    \"GA_RUN_REPEAT\": GA_RUN_REPEAT,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed8b42",
   "metadata": {},
   "source": [
    "##### Single GA loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53dec942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ga import GA\n",
    "from generator import Generator\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def run_single_ga_repeat(problem, mutations, population, config):\n",
    "    max_patience = config[\"GA_RUN_PATIENCE\"]\n",
    "    max_iterations = config[\"GA_MAX_RUN_ITERATIONS\"]\n",
    "\n",
    "    best_solution_cost = np.inf\n",
    "    patience = max_patience\n",
    "    num_iterations = 0\n",
    "\n",
    "    for mutation in MUTATIONS:\n",
    "        if mutation not in mutations:\n",
    "            mutation.proba = 0\n",
    "        else:\n",
    "            mutation.proba = 0.5\n",
    "\n",
    "    ga = GA(problem=problem, initial_population=population, C=1.2, alpha=0.9)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for state in ga.run(max_iter=max_iterations):\n",
    "        current_cost = ga.get_cost(state.solution)\n",
    "\n",
    "        if current_cost < best_solution_cost:\n",
    "            best_solution_cost = current_cost\n",
    "            patience = max_patience\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience <= 0:\n",
    "            break\n",
    "        num_iterations += 1\n",
    "\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    return {\n",
    "        \"cost\": best_solution_cost,\n",
    "        \"iterations\": num_iterations,\n",
    "        \"time\": elapsed_time,\n",
    "        \"cost_func_evals\": ga._cost_function_runs,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9459d75",
   "metadata": {},
   "source": [
    "##### Multiple GA loop job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f634eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_suite_for_testcase_remote(\n",
    "    testcase_name, problem, mutations, population, config\n",
    "):\n",
    "    repeats = config[\"GA_RUN_REPEAT\"]\n",
    "\n",
    "    futures = [\n",
    "        run_single_ga_repeat.remote(problem, mutations, population, config)\n",
    "        for _ in range(repeats)\n",
    "    ]\n",
    "\n",
    "    results = ray.get(futures)\n",
    "\n",
    "    best_costs = [r[\"cost\"] for r in results]\n",
    "    iterations_list = [r[\"iterations\"] for r in results]\n",
    "    times = [r[\"time\"] for r in results]\n",
    "    cost_func_evals = [r[\"cost_func_evals\"] for r in results]\n",
    "\n",
    "    stats = {\n",
    "        \"testcase\": testcase_name,\n",
    "        \"mutation_suite\": [m.__name__ for m in mutations],\n",
    "        \"cost_mean\": np.mean(best_costs),\n",
    "        \"cost_std\": np.std(best_costs),\n",
    "        \"cost_max\": max(best_costs),\n",
    "        \"cost_min\": min(best_costs),\n",
    "        \"cost_median\": np.median(best_costs),\n",
    "        \"iterations_mean\": np.mean(iterations_list),\n",
    "        \"iterations_std\": np.std(iterations_list),\n",
    "        \"iterations_min\": min(iterations_list),\n",
    "        \"iterations_max\": max(iterations_list),\n",
    "        \"iterations_median\": np.median(iterations_list),\n",
    "        \"time_mean\": np.mean(times),\n",
    "        \"time_std\": np.std(times),\n",
    "        \"time_min\": min(times),\n",
    "        \"time_max\": max(times),\n",
    "        \"time_median\": np.median(times),\n",
    "        \"cost_func_evals_min\": min(cost_func_evals),\n",
    "        \"cost_func_evals_max\": max(cost_func_evals),\n",
    "        \"cost_func_evals_mean\": np.mean(cost_func_evals),\n",
    "        \"cost_func_evals_std\": np.std(cost_func_evals),\n",
    "        \"cost_func_evals_median\": np.median(cost_func_evals),\n",
    "    }\n",
    "\n",
    "    stats = {k: round(v, 2) if isinstance(v, float) else v for k, v in stats.items()}\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec176b",
   "metadata": {},
   "source": [
    "### Mutation subset testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37853e",
   "metadata": {},
   "source": [
    "#### Subset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1cd1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "\n",
    "def get_all_subsets(lst):\n",
    "    return list(chain.from_iterable(combinations(lst, r) for r in range(len(lst) + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1206c1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutation_suite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UsedVM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[UnusedVM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[PM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>[UsedVM, UnusedVM, CM, RM, NewCM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[UsedVM, UnusedVM, PM, RM, NewCM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[UsedVM, CM, PM, RM, NewCM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[UnusedVM, CM, PM, RM, NewCM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[UsedVM, UnusedVM, CM, PM, RM, NewCM]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mutation_suite\n",
       "0                                      []\n",
       "1                                [UsedVM]\n",
       "2                              [UnusedVM]\n",
       "3                                    [CM]\n",
       "4                                    [PM]\n",
       "..                                    ...\n",
       "59      [UsedVM, UnusedVM, CM, RM, NewCM]\n",
       "60      [UsedVM, UnusedVM, PM, RM, NewCM]\n",
       "61            [UsedVM, CM, PM, RM, NewCM]\n",
       "62          [UnusedVM, CM, PM, RM, NewCM]\n",
       "63  [UsedVM, UnusedVM, CM, PM, RM, NewCM]\n",
       "\n",
       "[64 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutation_suite = get_all_subsets(MUTATIONS)\n",
    "\n",
    "mutations_suite_df = pd.DataFrame(\n",
    "    {\n",
    "        \"mutation_suite\": [\n",
    "            list(MUTATION_ALIAS[m.__name__] for m in mutations)\n",
    "            if len(mutations) > 0\n",
    "            else []\n",
    "            for mutations in mutation_suite\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "display(mutations_suite_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e48b3",
   "metadata": {},
   "source": [
    "#### Setup results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c477dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations_results_dir = os.path.join(results_dir, \"mutations\")\n",
    "os.makedirs(mutations_results_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72aa2e",
   "metadata": {},
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba01c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 13:10:56,777\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:19:56,749 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:20:56,750 E 378637 378637] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:21:56,753 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:22:56,755 E 378637 378637] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:23:56,756 E 378637 378637] (raylet) node_manager.cc:3287: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:24:56,757 E 378637 378637] (raylet) node_manager.cc:3287: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(run_suite_for_testcase_remote pid=386527)\u001b[0m [2025-06-14 13:25:05,728 E 386527 386563] core_worker.h:1617: Mismatched WorkerID: ignoring RPC for previous worker 8a888e8af3432ba5304f4e2cf2824861474d6c8a908370b25cc7d9e1, current worker ID: dacbdab6bcda2b3fda4df082ca5b1c2a565073cd5810bd0d2b5626c6\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:25:56,759 E 378637 378637] (raylet) node_manager.cc:3287: 20 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:26:56,760 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:27:56,761 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:28:56,763 E 378637 378637] (raylet) node_manager.cc:3287: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:29:56,764 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:30:56,766 E 378637 378637] (raylet) node_manager.cc:3287: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:31:56,768 E 378637 378637] (raylet) node_manager.cc:3287: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:32:56,771 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:33:56,772 E 378637 378637] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:34:56,774 E 378637 378637] (raylet) node_manager.cc:3287: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:35:56,775 E 378637 378637] (raylet) node_manager.cc:3287: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:36:56,776 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:37:56,778 E 378637 378637] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:38:56,779 E 378637 378637] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:40:56,782 E 378637 378637] (raylet) node_manager.cc:3287: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:41:56,783 E 378637 378637] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:42:56,785 E 378637 378637] (raylet) node_manager.cc:3287: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:43:56,787 E 378637 378637] (raylet) node_manager.cc:3287: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:44:56,788 E 378637 378637] (raylet) node_manager.cc:3287: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:45:56,790 E 378637 378637] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:50:56,798 E 378637 378637] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:51:56,800 E 378637 378637] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:52:56,801 E 378637 378637] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 13:53:56,804 E 378637 378637] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4b9ab173331f9c37ade5d87ce062bc14d16f0a06f427b703721e690b, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "reset_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0030049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c13c93cc6c4cc0a4853c5d30db80e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testcases:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3f7ac9b8748cf86bc0d7e7b63a0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mutations for 01-one-courier:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11a72f7858c4ebbb8b5e01583e473a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mutations for 02-four-couriers:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e261b50b61c4be08744b30bd39c8bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mutations for 03-big:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef6c480df3d487390f84d04fc6b00a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mutations for 04-medium:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37b21d150bb4773b85b82666d2c089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mutations for 05-small:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for testcase_name, problem in tqdm(data, desc=\"Testcases\"):\n",
    "    generator = Generator(problem=problem)\n",
    "    population = generator.generate_many_feasible(\n",
    "        num_to_find=GA_INITIAL_POPULATION_SIZE, max_attempts=1000, verbose=False\n",
    "    )\n",
    "\n",
    "    futures = []\n",
    "    metadata = []\n",
    "    for mutations in mutation_suite:\n",
    "        future = run_suite_for_testcase_remote.remote(\n",
    "            testcase_name, problem, mutations, population, config=CONFIG\n",
    "        )\n",
    "        futures.append(future)\n",
    "        metadata.append([m.__name__ for m in mutations])\n",
    "\n",
    "    testcase_results = []\n",
    "    with tqdm(total=len(futures), desc=f\"Mutations for {testcase_name}\") as pbar:\n",
    "        for i, future in enumerate(futures):\n",
    "            try:\n",
    "                res = ray.get(future)\n",
    "                testcase_results.append(res)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"ERROR in testcase {testcase_name}, mutations {metadata[i]}: {e}\"\n",
    "                )\n",
    "            pbar.update(1)\n",
    "\n",
    "    save_results(testcase_results, mutations_results_dir, \"mutations_stats\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31682836",
   "metadata": {},
   "source": [
    "### Populaiton size testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108b5e5",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42cf7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_MUTATIONS = [\n",
    "    PackagesMutation,\n",
    "    RouteMutation,\n",
    "    UsedVehiclesMutation,\n",
    "    UnusedVehiclesMutation,\n",
    "]\n",
    "\n",
    "POPULATION_SUITE = [10, 25, 50, 75, 100, 150, 200, 250, 300, 350, 400, 450, 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec8d09",
   "metadata": {},
   "source": [
    "#### Setup results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05c4ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_results_dir = os.path.join(results_dir, \"population\")\n",
    "os.makedirs(population_results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f538f7a",
   "metadata": {},
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98807923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 13:57:26,481\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:07:26,430 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:08:26,431 E 405882 405882] (raylet) node_manager.cc:3287: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:09:26,432 E 405882 405882] (raylet) node_manager.cc:3287: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:11:26,435 E 405882 405882] (raylet) node_manager.cc:3287: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:12:26,436 E 405882 405882] (raylet) node_manager.cc:3287: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:13:26,437 E 405882 405882] (raylet) node_manager.cc:3287: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:14:26,439 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:16:26,441 E 405882 405882] (raylet) node_manager.cc:3287: 11 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:18:26,444 E 405882 405882] (raylet) node_manager.cc:3287: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:20:26,446 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:24:26,451 E 405882 405882] (raylet) node_manager.cc:3287: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:25:26,452 E 405882 405882] (raylet) node_manager.cc:3287: 11 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:26:26,453 E 405882 405882] (raylet) node_manager.cc:3287: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:27:26,455 E 405882 405882] (raylet) node_manager.cc:3287: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:28:26,457 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:29:26,458 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:33:26,464 E 405882 405882] (raylet) node_manager.cc:3287: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:34:26,464 E 405882 405882] (raylet) node_manager.cc:3287: 21 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:35:26,466 E 405882 405882] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:37:26,468 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:38:26,469 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:44:26,476 E 405882 405882] (raylet) node_manager.cc:3287: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:45:26,477 E 405882 405882] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:46:26,478 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:47:26,480 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:51:26,486 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:52:26,487 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:54:26,489 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:55:26,490 E 405882 405882] (raylet) node_manager.cc:3287: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:56:26,491 E 405882 405882] (raylet) node_manager.cc:3287: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:57:26,492 E 405882 405882] (raylet) node_manager.cc:3287: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 14:59:26,494 E 405882 405882] (raylet) node_manager.cc:3287: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:00:26,495 E 405882 405882] (raylet) node_manager.cc:3287: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:02:26,497 E 405882 405882] (raylet) node_manager.cc:3287: 21 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:03:26,498 E 405882 405882] (raylet) node_manager.cc:3287: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:04:26,499 E 405882 405882] (raylet) node_manager.cc:3287: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:05:26,501 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:06:26,503 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:08:26,675 E 405882 405882] (raylet) node_manager.cc:3287: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:09:26,676 E 405882 405882] (raylet) node_manager.cc:3287: 24 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:10:26,677 E 405882 405882] (raylet) node_manager.cc:3287: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:14:26,681 E 405882 405882] (raylet) node_manager.cc:3287: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:20:26,688 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:22:26,690 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-14 15:24:26,692 E 405882 405882] (raylet) node_manager.cc:3287: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 67e50211ceda5add154495be4dedb79222251dbdd8f5c83fd01770fc, IP: 172.24.109.133) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.109.133`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "reset_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34540f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f564005730749b99c4423033140de7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testcases:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e66fb342a614a68a679dfdab3d989b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Population sizes for 01-one-courier:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2a0df17508462e8b3178935b2a6916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Population sizes for 02-four-couriers:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d4dea4cd1f4af9bdadf3520c5e9304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Population sizes for 03-big:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635975f11c4c4ba680dbeee256f41d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Population sizes for 04-medium:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad12d56972d742128a6063153e69f423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Population sizes for 05-small:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for testcase_name, problem in tqdm(data, desc=\"Testcases\"):\n",
    "    generator = Generator(problem=problem)\n",
    "\n",
    "    testcase_stats = []\n",
    "\n",
    "    for population_size in tqdm(\n",
    "        POPULATION_SUITE, desc=f\"Population sizes for {testcase_name}\"\n",
    "    ):\n",
    "        generator = Generator(problem=problem)\n",
    "        population = generator.generate_many_feasible(\n",
    "            num_to_find=population_size, max_attempts=100000, verbose=False\n",
    "        )\n",
    "\n",
    "        future = run_suite_for_testcase_remote.remote(\n",
    "            testcase_name, problem, SELECTED_MUTATIONS, population, config=CONFIG\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            res = ray.get(future)\n",
    "            res[\"population_size\"] = population_size\n",
    "            testcase_stats.append(res)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"ERROR in testcase {testcase_name}, population {population_size}: {e}\"\n",
    "            )\n",
    "\n",
    "    save_results(testcase_stats, population_results_dir, \"population_stats\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5aff7",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74433a98",
   "metadata": {},
   "source": [
    "### Testcase desctiption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_dir = \"problems/\"\n",
    "\n",
    "test_files = glob.glob(f\"{test_dir}/*.json\")\n",
    "\n",
    "\n",
    "def flatten_dict(d, parent_key=\"\", sep=\".\"):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_dict(v, new_key, sep=sep))\n",
    "        elif isinstance(v, list):\n",
    "            items[new_key] = len(v)\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ddac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_summary = []\n",
    "\n",
    "for f in test_files:\n",
    "    row = {\"testcase\": f.replace(\"problems/\", \"\").replace(\".json\", \"\")}\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        row.update(flatten_dict(data))\n",
    "        tests_summary.append(row)\n",
    "\n",
    "tests_desc_df = pd.DataFrame(tests_summary)\n",
    "\n",
    "if \"permissions\" in tests_desc_df.columns:\n",
    "    tests_desc_df.drop(columns=[\"permissions\"], inplace=True)\n",
    "\n",
    "tests_desc_df.sort_values(by=\"testcase\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015da768",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_desc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff712e0d",
   "metadata": {},
   "source": [
    "### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(directory, filename_suffix=\"\"):\n",
    "    files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    results = {}\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        testcase_name = (\n",
    "            os.path.basename(file).replace(\".csv\", \"\").replace(filename_suffix, \"\")\n",
    "        )\n",
    "        results[testcase_name] = df\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results/\"\n",
    "\n",
    "mutations_results_dir = os.path.join(results_dir, \"mutations\")\n",
    "population_results_dir = os.path.join(results_dir, \"population\")\n",
    "\n",
    "mutation_results = load_results(mutations_results_dir, \"mutation_stats\")\n",
    "population_results = load_results(population_results_dir, \"population_stats\")\n",
    "\n",
    "print(mutation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32077b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats_subplot(ax, stat_cols):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
